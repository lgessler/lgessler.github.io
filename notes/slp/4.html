<!doctype html>
<html lang="en">
<head>
<title>Speech and Language Processing 3ed, Chapter 4, Language Modeling with N-grams</title>
<!-- 2018-08-25 Sat 01:53 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="generator" content="Org-mode">
<meta name="author" content="Luke Gessler">

<link  href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.5/js/bootstrap.min.js"></script>
<style type="text/css">
/* org mode styles on top of twbs */

html {
    position: relative;
    min-height: 100%;
}

body {
    font-size: 18px;
    margin-bottom: 105px;
}

footer {
    position: absolute;
    bottom: 0;
    width: 100%;
    height: 101px;
    background-color: #f5f5f5;
}

footer > div {
    padding: 10px;
}

footer p {
    margin: 0 0 5px;
    text-align: center;
    font-size: 16px;
}

#table-of-contents {
    margin-top: 20px;
    margin-bottom: 20px;
}

blockquote p {
    font-size: 18px;
}

pre {
    font-size: 16px;
}

.footpara {
    display: inline-block;
}

figcaption {
  font-size: 16px;
  color: #666;
  font-style: italic;
  padding-bottom: 15px;
}

/* from twbs docs */

.bs-docs-sidebar.affix {
    position: static;
}
@media (min-width: 768px) {
    .bs-docs-sidebar {
        padding-left: 20px;
    }
}

/* All levels of nav */
.bs-docs-sidebar .nav > li > a {
    display: block;
    padding: 4px 20px;
    font-size: 14px;
    font-weight: 500;
    color: #999;
}
.bs-docs-sidebar .nav > li > a:hover,
.bs-docs-sidebar .nav > li > a:focus {
    padding-left: 19px;
    color: #A1283B;
    text-decoration: none;
    background-color: transparent;
    border-left: 1px solid #A1283B;
}
.bs-docs-sidebar .nav > .active > a,
.bs-docs-sidebar .nav > .active:hover > a,
.bs-docs-sidebar .nav > .active:focus > a {
    padding-left: 18px;
    font-weight: bold;
    color: #A1283B;
    background-color: transparent;
    border-left: 2px solid #A1283B;
}

/* Nav: second level (shown on .active) */
.bs-docs-sidebar .nav .nav {
    display: none; /* Hide by default, but at >768px, show it */
    padding-bottom: 10px;
}
.bs-docs-sidebar .nav .nav > li > a {
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 30px;
    font-size: 12px;
    font-weight: normal;
}
.bs-docs-sidebar .nav .nav > li > a:hover,
.bs-docs-sidebar .nav .nav > li > a:focus {
    padding-left: 29px;
}
.bs-docs-sidebar .nav .nav > .active > a,
.bs-docs-sidebar .nav .nav > .active:hover > a,
.bs-docs-sidebar .nav .nav > .active:focus > a {
    padding-left: 28px;
    font-weight: 500;
}

/* Nav: third level (shown on .active) */
.bs-docs-sidebar .nav .nav .nav {
    padding-bottom: 10px;
}
.bs-docs-sidebar .nav .nav .nav > li > a {
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 40px;
    font-size: 12px;
    font-weight: normal;
}
.bs-docs-sidebar .nav .nav .nav > li > a:hover,
.bs-docs-sidebar .nav .nav .nav > li > a:focus {
    padding-left: 39px;
}
.bs-docs-sidebar .nav .nav .nav > .active > a,
.bs-docs-sidebar .nav .nav .nav > .active:hover > a,
.bs-docs-sidebar .nav .nav .nav > .active:focus > a {
    padding-left: 38px;
    font-weight: 500;
}

/* Show and affix the side nav when space allows it */
@media (min-width: 992px) {
    .bs-docs-sidebar .nav > .active > ul {
        display: block;
    }
    /* Widen the fixed sidebar */
    .bs-docs-sidebar.affix,
    .bs-docs-sidebar.affix-bottom {
        width: 213px;
    }
    .bs-docs-sidebar.affix {
        position: fixed; /* Undo the static from mobile first approach */
        top: 20px;
    }
    .bs-docs-sidebar.affix-bottom {
        position: absolute; /* Undo the static from mobile first approach */
    }
    .bs-docs-sidebar.affix .bs-docs-sidenav,.bs-docs-sidebar.affix-bottom .bs-docs-sidenav {
        margin-top: 0;
        margin-bottom: 0
    }
}
@media (min-width: 1200px) {
    /* Widen the fixed sidebar again */
    .bs-docs-sidebar.affix-bottom,
    .bs-docs-sidebar.affix {
        width: 263px;
    }
}
</style>
<script type="text/javascript">
$(function() {
    'use strict';

    $('.bs-docs-sidebar li').first().addClass('active');

    $(document.body).scrollspy({target: '.bs-docs-sidebar'});

    $('.bs-docs-sidebar').affix();
});
</script><link rel='stylesheet' type='text/css' href='/css/styles.css'>
            <link href="https://fonts.googleapis.com/css?family=Roboto"
                  rel="stylesheet">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  displayAlign: "center",
  displayIndent: "2em",
  messageStyle: "none",
  "HTML-CSS": {
    scale: 100,
    styles: {
      ".MathJax_Display": {
        "font-size": "100%"
      }
    }
  },
  "SVG": {
    scale: 100,
    styles: {
      ".MathJax_SVG_Display": {
        "font-size": "100%",
        "margin-left": "-2.281em"
      }
    }
  }
});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"></script>
</head>
<body>
<div id="preamble" class="">
<div class="container"><div class="row"><div class="col-md-9"><div style="font-size:11px;text-align:center;"><a style="color:gray;" href="/">Home</a> <a style="color:gray;" href="/posts/">Blog</a></div><div style="margin-top:0px;margin-bottom:-12px;color:#a0a0a0;">Luke Gessler</div></div></div></div>
</div>
<div id="content" class="container">
<div class="row"><div class="col-md-9"><h1 class="title">Speech and Language Processing 3ed, Chapter 4, Language Modeling with N-grams</h1>
<style>body { max-width: 700px; } li { padding-bottom: 20px; }</style>

<div id="outline-container-sec-" class="outline-2">
<h2 id="sec-">Notes</h2>
<div class="outline-text-2" id="text-">
</div><div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">4.1 N-grams</h3>
<div class="outline-text-3" id="text-">
<ul class="org-ul">
<li>We often want to calculate \(P(w_1, \ldots, w_n)\) or \(P(w_n | w_1, \ldots, w_{n-1})\)
</li>
<li>One way for the latter: calculate \[\frac{C(w_1, \ldots, w_n)}{C(w_1, \ldots, w_{n-1})}\] in some corpus. But this is tiny/zero for all but the smallest values of \(n\)
</li>
<li>Another way: try the chain rule \[P(w_1, \ldots, w_n) = \prod_{k=1}^n P(w_k | w_1, \ldots, w_{k-1})\] But what's \(P(w_k|w_1 \ldots w_{k-1})\)? No obvious answer
</li>
<li>One way to simplify it: make Markov assumption that the probability of a word only depends on the word before it: \( P(w_n|w_1 \ldots w_{n-1}) \approx P(w_n | w_{n-1}) \)
</li>
<li>Markov assumption uses bigrams, generalizable to n-grams:  \[ P(w_n|w_1 \ldots w_{n-1}) \approx P(w_n | w_{n-N+1}, \ldots, w_{n-1}) \]
</li>
<li>How could you get \( P(w_n|w_{n-1}) \)? Maximum likelihood estimation equates this to \[ \frac{C(w_{n-1}, w_n)}{\sum_{v \in W} C(w_{n-1}v)} = \frac{C(w_{n-1}, w_n)}{C(w_{n-1})} \]
</li>
<li>When \( N > 1 \), sentences in the corpus need to be padded with special sentence boundary symbols e.g. <code>&lt;s&gt; &lt;s&gt; I am sam &lt;/s&gt; &lt;/s&gt;</code> for a trigram.
</li>
<li>Bigrams and trigrams are common, 4-grams and 5-grams uncommon because of sparsity
</li>
<li>Log probabilities used to avoid underflow
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">4.2 Evaluating language models</h3>
<div class="outline-text-3" id="text-">
<ul class="org-ul">
<li>Two kinds: intrinsic evaluation measures model quality independent of any "application", extrinsic evaluation measures based on some kind of application 
</li>
<li>Intrinsic evaluation requires a test set separate from the training set
</li>
<li>One kind of intrinsic evaluation: the likelihood of the test set
</li>
<li>Perplexity: inverse likelihood of the test set, normalized by word number. Suppose a bigram model and a test set \( W = w_1, \ldots, w_N \) where \(N\) is the size of the test set (and not necessarily the size of the vocabulary):
</li>
</ul>

<p>
\[ PP(W) = \sqrt[N]{\prod_{i=1}^N \frac{1}{P(w_i|w_{i-1})}} \]
</p>

<ul class="org-ul">
<li>Perplexity can also be thought of as weighted average branching factor 
</li>
<li>Perplexity best used as a quick check, for real quality checking you need an extrinsic test
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">4.3 Generalization and zeros</h3>
<div class="outline-text-3" id="text-">
<ul class="org-ul">
<li>Different domains have domain-specific language
</li>
<li>Different domain-specific languages may not overlap very much (Shakespeare vs. newspaper corpora probably have little) 
</li>
<li>Need to make sure your training set matches your target set's <i>genre</i>
</li>
<li>Increasing \(N\) improves performance, but makes the probability space explode
</li>
<li>N-grams that did not appear in our training set but are actually valid strings become more common
</li>
<li>One way to combat the sparsity: close the vocabulary, treat anything that isn't in the vocabulary as a single n-gram type
</li>
<li>Caution: perplexity can be hacked if vocabulary becomes small. (Most n-grams are assigned to the unknown type.)
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">4.4 Smoothing</h3>
<div class="outline-text-3" id="text-">
<ul class="org-ul">
<li>We can do better than assigning every previously-unseen n-gram to an unknown type
</li>
<li>Laplace smoothing: 
<ul class="org-ul">
<li>add one to all counts, normalize. (N is the number of word tokens, V is the number of word types) \[ P_\text{Laplace}(w_i) = \frac{c_i + 1}{N + V} \]
</li>
<li>Interesting to find the adjusted count: \[c_i^* = (c_i + 1)\frac{N}{N + V}\]
</li>
<li>Redistributes too much mass to zeros
</li>
</ul>
</li>
<li>Add-k smoothing: generalization of Laplace smoothing for \(k\) instead of 1.
</li>
<li>Backoff and interpolation
<ul class="org-ul">
<li>Simple backoff: if \(N\)-gram is zero, check if \(N-1\)-gram isn't zero, etc.
</li>
<li>Interpolation: linear mix of 1-Ngrams. For trigram: \[ \hat{P}(w_n|w_{n-1},w_{n-2}) = \lambda_1 P(w_n|w_{n-2},w_{n-1}) + \lambda_2 P(w_n|w_{n-1}) + \lambda_3 P(w_n) \\\sum_i \lambda_i = 1 \]
</li>
<li>\(\lambda\) values trained on a held-out corpus, another, separate training corpus
</li>
</ul>
</li>
<li>Katz backoff
<ul class="org-ul">
<li>for backoff to preserve correct probability distribution, need to take mass away from higher n-gram distributions
</li>
<li>mass is taken away from the higher order n-grams, given via a function \(\alpha\) to lower-order n-grams during the recursive backoff step
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">4.5 Kneser-Ney smoothing</h3>
<div class="outline-text-3" id="text-">
<ul class="org-ul">
<li>Absolute discounting: fixed amount is subtracted from all counts, interpolate with a lower-order model \[P(w_i|w_{i-1}) = \frac{\text{max}(C(w_{i-1}, w_i) - d, 0)}{C(w_{i-1})} + \lambda(w_{i-1})P(w_i) \] (note that the \(\lambda\) term must redistribute the mass to preserve the integrity of the distribution)
</li>
<li>K-N based on absolute discounting, but uses a specialized unigram model
</li>
<li>Intuition: some unigrams are very common but only appear in a few fixed phrases, like Kong
</li>
<li>To find \(P_\text{continuation}(w)\), find all word types that precede \(w\) and normalize by the total number of word bigram types: \[ P_\text{continuation}(w) = \frac{|\{v:C(vw)>0\}|}{|\{(a,b):C(a,b) > 0\}|} \]
</li>
<li>Frequent word with few phrases it appears in like Kong will have low continuation
</li>
<li>Final K-N formula is a modified version of absolute discounting: \[P(w_i|w_{i-1}) = \frac{\text{max}(C(w_{i-1}, w_i) - d, 0)}{C(w_{i-1})} + \lambda(w_{i-1})P_\text{continuation}(w_i) \] 
</li>
<li>The \(\lambda\) term normalizes the mass
</li>
<li>K-N uses a recursive formulation with a \(\lambda\) term adjusting each recursive call
</li>
<li>Normal count used for the highest order, continuation count used for lower orders
</li>
<li>Modified Kneser-Ney uses different discount for each value of \(n\)
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">The web and stupid backoff</h3>
<div class="outline-text-3" id="text-">
<ul class="org-ul">
<li>Various tricks and approximations make web scale LM's feasible, incl. Bloom filters
</li>
<li>For very large LM's, K-N is possible but stupid backoff shown to be sufficient
</li>
<li>Stupid backoff 
<ul class="org-ul">
<li>gives up modeling a true probability distribution
</li>
<li>No discounting higher-order probabilities
</li>
<li>If backoff needed, back off, but penalize it with some constant \(\lambda \in [0,1]\)
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>



<div id="outline-container-sec-" class="outline-2">
<h2 id="sec-">Exercises</h2>
<div class="outline-text-2" id="text-">
</div><div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">4.1</h3>
<div class="outline-text-3" id="text-">
<p>
Formula for trigram probability:
</p>

<p>
\( P(w_n|w_{n-2}w_{n-1}) = \frac{C(w_{n-2}w_{n-1}w_n)}{C(w_{n-2}w_{n-1})} \)
</p>

<p>
Bigram counts:
</p>

<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="right">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">seq</th>
<th scope="col" class="text-right">freq</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">&lt;s&gt; &lt;s&gt;</td>
<td class="text-right">3</td>
</tr>

<tr>
<td class="text-left">&lt;s&gt; I</td>
<td class="text-right">2</td>
</tr>

<tr>
<td class="text-left">I am</td>
<td class="text-right">2</td>
</tr>

<tr>
<td class="text-left">am Sam</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">Sam &lt;/s&gt;</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">&lt;/s&gt; &lt;/s&gt;</td>
<td class="text-right">3</td>
</tr>

<tr>
<td class="text-left">&lt;s&gt; Sam</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">Sam I</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">am &lt;/s&gt;</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">I do</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">do not</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">not like</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">like green</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">green eggs</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">eggs and</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">and ham</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">ham &lt;/s&gt;</td>
<td class="text-right">1</td>
</tr>
</tbody>
</table>

<p>
Trigram probabilities: 
</p>
<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="right">

<col  class="left">

<col  class="right">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">Trigram</th>
<th scope="col" class="text-right">Count</th>
<th scope="col" class="text-left">Trigram probability</th>
<th scope="col" class="text-right">Val</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">&lt;s&gt; &lt;s&gt; I</td>
<td class="text-right">2</td>
<td class="text-left">P(I &vert; &lt;s&gt; &lt;s&gt;)</td>
<td class="text-right">2/3</td>
</tr>

<tr>
<td class="text-left">&lt;s&gt; I am</td>
<td class="text-right">1</td>
<td class="text-left">P(am &vert; &lt;s&gt; I)</td>
<td class="text-right">1/2</td>
</tr>

<tr>
<td class="text-left">I am Sam</td>
<td class="text-right">1</td>
<td class="text-left">P(Sam &vert; I am)</td>
<td class="text-right">1/2</td>
</tr>

<tr>
<td class="text-left">am Sam &lt;/s&gt;</td>
<td class="text-right">1</td>
<td class="text-left">P(&lt;/s&gt; &vert; am Sam)</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">Sam &lt;/s&gt; &lt;/s&gt;</td>
<td class="text-right">1</td>
<td class="text-left">P(&lt;/s&gt; &vert; Sam &lt;/s&gt;)</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">&lt;s&gt; &lt;s&gt; Sam</td>
<td class="text-right">1</td>
<td class="text-left">P(Sam &vert; &lt;s&gt; &lt;s&gt;)</td>
<td class="text-right">1/3</td>
</tr>

<tr>
<td class="text-left">&lt;s&gt; Sam I</td>
<td class="text-right">1</td>
<td class="text-left">P(I &vert; &lt;s&gt; Sam)</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">Sam I am</td>
<td class="text-right">1</td>
<td class="text-left">P(am &vert; Sam I)</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">I am &lt;/s&gt;</td>
<td class="text-right">1</td>
<td class="text-left">P(&lt;/s&gt; &vert; I am)</td>
<td class="text-right">1/2</td>
</tr>

<tr>
<td class="text-left">am &lt;/s&gt; &lt;/s&gt;</td>
<td class="text-right">1</td>
<td class="text-left">P(&lt;/s&gt; &vert; am &lt;/s&gt;)</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">&lt;s&gt; I do</td>
<td class="text-right">1</td>
<td class="text-left">P(do &vert; &lt;s&gt; I)</td>
<td class="text-right">1/2</td>
</tr>

<tr>
<td class="text-left">I do not</td>
<td class="text-right">1</td>
<td class="text-left">P(not &vert; I do)</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">do not like</td>
<td class="text-right">1</td>
<td class="text-left">P(like &vert; do not)</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">not like green</td>
<td class="text-right">1</td>
<td class="text-left">P(green &vert; not like)</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">like green eggs</td>
<td class="text-right">1</td>
<td class="text-left">P(eggs &vert; like green)</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">green eggs and</td>
<td class="text-right">1</td>
<td class="text-left">P(and &vert; green eggs)</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">eggs and ham</td>
<td class="text-right">1</td>
<td class="text-left">P(ham &vert; eggs and)</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">and ham &lt;/s&gt;</td>
<td class="text-right">1</td>
<td class="text-left">P(&lt;/s&gt; &vert; and ham)</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">ham &lt;/s&gt; &lt;/s&gt;</td>
<td class="text-right">1</td>
<td class="text-left">P(&lt;/s&gt; &vert; ham &lt;/s&gt;)</td>
<td class="text-right">1</td>
</tr>
</tbody>
</table>
</div>
</div>


<div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">4.2</h3>
<div class="outline-text-3" id="text-">
<p>
According to fig. 4.2, 
</p>

<p>
P(i want chinese food) 
</p>

<p>
= P(&lt;s&gt; i want chinese food &lt;/s&gt;) 
</p>

<p>
= P(i | &lt;s&gt;) P(want | i) P(chinese | want) P(food | chinese) P(&lt;/s&gt; | food)
</p>

<p>
= 0.25 * 0.33 * 0.0065 * 0.52 * 0.68
</p>

<p>
= 1.9e-4
</p>

<p>
According to fig 4.6 (note: first and last probabilities are reused from before since they're not given again in 4.6):
</p>

<p>
= P(i | &lt;s&gt;) P(want | i) P(chinese | want) P(food | chinese) P(&lt;/s&gt; | food)
</p>

<p>
= 0.25 * 0.21 * 0.0029 * 0.052 * 0.68
</p>

<p>
= 5.4e-6
</p>
</div>
</div>

<div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">4.3</h3>
<div class="outline-text-3" id="text-">
<p>
Unsmoothed is higher. Intuitively, additive smoothing takes probability mass proportionate to the amount of mass a certain outcome has, making less-seen outcomes more likely and more-seen outcomes less likely. We can see that P(food | chinese), which used to have <i>p = 0.52</i>, was discounted by a factor of ten! On the other hand, P(chinese | want), which had the lower probability <i>p = 0.0065</i>, had <i>p = 0.0029</i> after smoothing. 
</p>

<p>
In the sense that the unsmoothed model assigns it a higher probability than the smoothed model, the sentence "I want chinese food" is a "likely" sentence. If we had encountered an "unlikely" sentence such as "I want roasted zucchini skin and", we would have expected the opposite: the unsmoothed model would have assigned it a lower probability.
</p>
</div>
</div>

<div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">4.4</h3>
<div class="outline-text-3" id="text-">
<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="right">

<col  class="left">

<col  class="right">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">bigram</th>
<th scope="col" class="text-right">raw count</th>
<th scope="col" class="text-left">unigram</th>
<th scope="col" class="text-right">raw count</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">&lt;s&gt; I</td>
<td class="text-right">3</td>
<td class="text-left">&lt;s&gt;</td>
<td class="text-right">4</td>
</tr>

<tr>
<td class="text-left">I am</td>
<td class="text-right">3</td>
<td class="text-left">&lt;/s&gt;</td>
<td class="text-right">4</td>
</tr>

<tr>
<td class="text-left">am Sam</td>
<td class="text-right">2</td>
<td class="text-left">I</td>
<td class="text-right">4</td>
</tr>

<tr>
<td class="text-left">Sam &lt;/s&gt;</td>
<td class="text-right">2</td>
<td class="text-left">am</td>
<td class="text-right">3</td>
</tr>

<tr>
<td class="text-left">&lt;s&gt; Sam</td>
<td class="text-right">1</td>
<td class="text-left">Sam</td>
<td class="text-right">4</td>
</tr>

<tr>
<td class="text-left">Sam I</td>
<td class="text-right">1</td>
<td class="text-left">do</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">am &lt;/s&gt;</td>
<td class="text-right">1</td>
<td class="text-left">not</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">I do</td>
<td class="text-right">1</td>
<td class="text-left">like</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">do not</td>
<td class="text-right">1</td>
<td class="text-left">green</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">not like</td>
<td class="text-right">1</td>
<td class="text-left">eggs</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">like green</td>
<td class="text-right">1</td>
<td class="text-left">and</td>
<td class="text-right">1</td>
</tr>

<tr>
<td class="text-left">green eggs</td>
<td class="text-right">1</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
</tr>

<tr>
<td class="text-left">eggs and</td>
<td class="text-right">1</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
</tr>

<tr>
<td class="text-left">and Sam</td>
<td class="text-right">1</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
By formula 4.21, P(Sam | am) = \(\frac{C(\texttt{am sam}) + 1}{C(\texttt{am}) + V}\) = \( \frac{2 + 1}{3 + 11} \) = \( \frac{3}{14} \)
</p>
</div>
</div>

<div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">4.5</h3>
<div class="outline-text-3" id="text-">
<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="right">

<col  class="left">

<col  class="right">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">bigram</th>
<th scope="col" class="text-right">count</th>
<th scope="col" class="text-left">unigram</th>
<th scope="col" class="text-right">count</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">&lt;s&gt; a</td>
<td class="text-right">2</td>
<td class="text-left">&lt;s&gt;</td>
<td class="text-right">4</td>
</tr>

<tr>
<td class="text-left">&lt;s&gt; b</td>
<td class="text-right">2</td>
<td class="text-left">a</td>
<td class="text-right">4</td>
</tr>

<tr>
<td class="text-left">a a</td>
<td class="text-right">1</td>
<td class="text-left">b</td>
<td class="text-right">4</td>
</tr>

<tr>
<td class="text-left">a b</td>
<td class="text-right">1</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
</tr>

<tr>
<td class="text-left">b b</td>
<td class="text-right">1</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
</tr>

<tr>
<td class="text-left">b a</td>
<td class="text-right">1</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
</tr>
</tbody>
</table>

<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="left">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">probability</th>
<th scope="col" class="text-left">value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">P(a &vert; &lt;s&gt;)</td>
<td class="text-left">1/2</td>
</tr>

<tr>
<td class="text-left">P(b &vert; &lt;s&gt;)</td>
<td class="text-left">1/2</td>
</tr>

<tr>
<td class="text-left">P(a &vert; a)</td>
<td class="text-left">1/4</td>
</tr>

<tr>
<td class="text-left">P(b &vert; a)</td>
<td class="text-left">1/4</td>
</tr>

<tr>
<td class="text-left">P(b &vert; b)</td>
<td class="text-left">1/4</td>
</tr>

<tr>
<td class="text-left">P(a &vert; b)</td>
<td class="text-left">1/4</td>
</tr>
</tbody>
</table>


<p>
P(a a) = P(a|&lt;s&gt;) P(a|a) 
</p>

<p>
= \( \frac{C(\lt\text{s}\gt \text{a})}{C(\lt\text{s}\gt)} \times \frac{\text{C(a a)}}{\text{C(a)}} \) 
</p>

<p>
= 1/2 * 1/4 = 1/8
</p>

<p>
P(a b) = P(a | &lt;s&gt;) * P(b | a) = 1/2 * 1/4 = 1/8
</p>

<p>
P(b a) = P(b | &lt;s&gt;) * P(a | b) = 1/2 * 1/4 = 1/8
</p>

<p>
P(b b) = P(b | &lt;s&gt;) * P(b | b) = 1/2 * 1/4 = 1/8
</p>

<p>
Since we know P(a | &lt;s&gt;) = P(b | &lt;s&gt;) and P(a | b) = P(a | a) = P(b | a) = P(b | b), we know the sum probability of all three-letter sentences is \(\sum_1^{2^3} \frac{1}{16} = \frac{1}{2} \)
</p>

<p>
Hmm&#x2026; these both add up to 1/2 instead of 1 like the book says they should, but this is still far enough to demonstrate what the book wanted us to: that the sum of probabilities across all sentences does not add up to 1.
</p>
</div>
</div>

<div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">4.6</h3>
<div class="outline-text-3" id="text-">
\begin{array}{lll}
P(w_3|w_1,w_2) & = \frac{C(w_1, w_2, w_3) + 1}{C(w_1, w_2) + V^2}
\end{array}

<p>
Note that we have to square <i>V</i> because that is the total number of bigrams.
</p>
</div>
</div>

<div id="outline-container-sec-" class="outline-3">
<h3 id="sec-">4.7</h3>
<div class="outline-text-3" id="text-">
<p>
With this approach:
</p>

\begin{array}{lll}
P(w_2|w_1) &=& \lambda_1P(w_2|w_1) + \lambda_2P(w_1) \\

&=& \lambda_1 \frac{C(w_1w_2)}{C(w_1)} + \lambda_2 \frac{C(w_1)}{\sum_i C(w_i)}
\end{array}

<p>
For our specific words,
</p>


\begin{array}{lll}
P(\text{Sam}|\text{am}) &=& \lambda_1 \frac{C(\text{am Sam})}{C(\text{am})} + \lambda_2 \frac{C(\text{am})}{\sum_i C(w_i)} \\

&=& \frac{1}{2} \frac{2}{3} + \frac{1}{2} \frac{3}{25} \\

&=& \frac{118}{300}
\end{array}
</div>
</div>
</div>
</div><div class="col-md-3"><nav id="table-of-contents">
<div id="text-table-of-contents" class="bs-docs-sidebar">
<ul class="nav">
<li><a href="#sec-">Notes</a>
<ul class="nav">
<li><a href="#sec-">4.1 N-grams</a></li>
<li><a href="#sec-">4.2 Evaluating language models</a></li>
<li><a href="#sec-">4.3 Generalization and zeros</a></li>
<li><a href="#sec-">4.4 Smoothing</a></li>
<li><a href="#sec-">4.5 Kneser-Ney smoothing</a></li>
<li><a href="#sec-">The web and stupid backoff</a></li>
</ul>
</li>
<li><a href="#sec-">Exercises</a>
<ul class="nav">
<li><a href="#sec-">4.1</a></li>
<li><a href="#sec-">4.2</a></li>
<li><a href="#sec-">4.3</a></li>
<li><a href="#sec-">4.4</a></li>
<li><a href="#sec-">4.5</a></li>
<li><a href="#sec-">4.6</a></li>
<li><a href="#sec-">4.7</a></li>
</ul>
</li>
</ul>
</div>
</nav>
</div></div></div>
<footer id="postamble" class="">
<div class="container"><div class="row"><div class="col-md-9"><div id="disqus_thread"></div></div></div></div>
</footer>
</body>
</html>
