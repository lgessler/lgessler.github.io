@inproceedings{marashian-etal-2025-priest,
    title = "From Priest to Doctor: Domain Adaptation for Low-Resource Neural Machine Translation",
    author = "Marashian, Ali  and
      Rice, Enora  and
      Gessler, Luke  and
      Palmer, Alexis  and
      von der Wense, Katharina",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.472/",
    pages = "7087--7098",
    abstract = "Many of the world`s languages have insufficient data to train high-performing general neural machine translation (NMT) models, let alone domain-specific models, and often the only available parallel data are small amounts of religious texts. Hence, domain adaptation (DA) is a crucial issue faced by contemporary NMT and has, so far, been underexplored for low-resource languages. In this paper, we evaluate a set of methods from both low-resource NMT and DA in a realistic setting, in which we aim to translate between a high-resource and a low-resource language with access to only: a) parallel Bible data, b) a bilingual dictionary, and c) a monolingual target-domain corpus in the high-resource language. Our results show that the effectiveness of the tested methods varies, with the simplest one, DALI, being most effective. We follow up with a small human evaluation of DALI, which shows that there is still a need for more careful investigation of how to accomplish DA for low-resource NMT.",
    keywords = {conf}
}

@article{10.1162/coli_a_00538,
    author = {Zeldes, Amir and Aoyama, Tatsuya and Liu, Yang Janet and Peng, Siyao and Das, Debopam and Gessler, Luke},
    title = {e{RST}: A Signaled Graph Theory of Discourse Relations and Organization},
    journal = {Computational Linguistics},
    pages = {1-50},
    year = {2024},
    month = {11},
    abstract = {In this article we present Enhanced Rhetorical Structure Theory (eRST), a new theoretical framework for computational discourse analysis, based on an expansion of Rhetorical Structure Theory (RST). The framework encompasses discourse relation graphs with tree-breaking, non-projective and concurrent relations, as well as implicit and explicit signals which give explainable rationales to our analyses. We survey shortcomings of RST and other existing frameworks, such as Segmented Discourse Representation Theory, the Penn Discourse Treebank, and Discourse Dependencies, and address these using constructs in the proposed theory. We provide annotation, search, and visualization tools for data, and present and evaluate a freely available corpus of English annotated according to our framework, encompassing 12 spoken and written genres with over 200K tokens. Finally, we discuss automatic parsing, evaluation metrics, and applications for data in our framework.},
    issn = {0891-2017},
    doi = {10.1162/coli_a_00538},
    url = {https://doi.org/10.1162/coli\_a\_00538},
    eprint = {https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli\_a\_00538/2479678/coli\_a\_00538.pdf}
}

@inproceedings{gessler-2024-pronto,
    title = "{P}r{O}nto: Language Model Evaluations for 859 Languages",
    author = "Gessler, Luke",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA \& ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1159",
    pages = "13243--13256",
    abstract = "Evaluation datasets are critical resources for measuring the quality of pretrained language models. However, due to the high cost of dataset annotation, these resources are scarce for most languages other than English, making it difficult to assess the quality of language models. In this work, we present a new method for evaluation dataset construction which enables any language with a New Testament translation to receive a suite of evaluation datasets suitable for pretrained language model evaluation. The method critically involves aligning verses with those in the New Testament portion of English OntoNotes, and then projecting annotations from English to the target language, with no manual annotation required. We apply this method to 1051 New Testament translations in 859 languages and make them publicly available. Additionally, we conduct experiments which demonstrate the efficacy of our method for creating evaluation tasks which can assess language model quality.",
    keywords = {conf},
}

@inproceedings{gessler-von-der-wense-2024-nlp,
    title = "{NLP} for Language Documentation: Two Reasons for the Gap between Theory and Practice",
    author = "Gessler, Luke  and
      {von der Wense}, Katharina",
    editor = "Mager, Manuel  and
      Ebrahimi, Abteen  and
      Rijhwani, Shruti  and
      Oncevay, Arturo  and
      Chiruzzo, Luis  and
      Pugh, Robert  and
      {von der Wense}, Katharina",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.americasnlp-1.1",
    doi = "10.18653/v1/2024.americasnlp-1.1",
    pages = "1--6",
    abstract = "Both NLP researchers and linguists have expressed a desire to use language technologies in language documentation, but most documentary work still proceeds without them, presenting a lost opportunity to hasten the preservation of the world{'}s endangered languages, such as those spoken in Latin America. In this work, we empirically measure two factors that have previously been identified as explanations of this low utilization: curricular offerings in graduate programs, and rates of interdisciplinary collaboration in publications related to NLP in language documentation. Our findings verify the claim that interdisciplinary training and collaborations are scarce and support the view that interdisciplinary curricular offerings facilitate interdisciplinary collaborations.",
    keywords = {workshop},
}

@inproceedings{rice-etal-2024-tams,
    title = "{TAMS}: Translation-Assisted Morphological Segmentation",
    author = "Rice, Enora  and
      Marashian, Ali  and
      Gessler, Luke  and
      Palmer, Alexis  and
      {von der Wense}, Katharina",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.366",
    pages = "6752--6765",
    abstract = "Canonical morphological segmentation is the process of analyzing words into the standard (\textit{aka} underlying) forms of their constituent morphemes.This is a core task in endangered language documentation, and NLP systems have the potential to dramatically speed up this process. In typical language documentation settings, training data for canonical morpheme segmentation is scarce, making it difficult to train high quality models. However, translation data is often much more abundant, and, in this work, we present a method that attempts to leverage translation data in the canonical segmentation task. We propose a character-level sequence-to-sequence model that incorporates representations of translations obtained from pretrained high-resource monolingual language models as an additional signal. Our model outperforms the baseline in a super-low resource setting but yields mixed results on training splits with more data. Additionally, we find that we can achieve strong performance even without needing difficult-to-obtain word level alignments. While further work is needed to make translations useful in higher-resource settings, our model shows promise in severely resource-constrained settings.",
    keywords = {conf},
}

@inproceedings{gessler-schneider-2023-syntactic,
    title = "Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?",
    author = "Gessler, Luke  and
      Schneider, Nathan",
    editor = "Jiang, Jing  and
      Reitter, David  and
      Deng, Shumin",
    booktitle = "Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-1.17",
    doi = "10.18653/v1/2023.conll-1.17",
    pages = "238--253",
    abstract = "A line of work on Transformer-based language models such as BERT has attempted to use syntactic inductive bias to enhance the pretraining process, on the theory that building syntactic structure into the training process should reduce the amount of data needed for training. But such methods are often tested for high-resource languages such as English. In this work, we investigate whether these methods can compensate for data sparseness in low-resource languages, hypothesizing that they ought to be more effective for low-resource languages. We experiment with five low-resource languages: Uyghur, Wolof, Maltese, Coptic, and Ancient Greek. We find that these syntactic inductive bias methods produce uneven results in low-resource settings, and provide surprisingly little benefit in most cases.",
    keywords = {conf},
}

@inproceedings{aoyama-etal-2023-gentle,
    title = "{GENTLE}: A Genre-Diverse Multilayer Challenge Set for {E}nglish {NLP} and Linguistic Evaluation",
    author = "Aoyama, Tatsuya  and
      Behzad, Shabnam  and
      Gessler, Luke  and
      Levine, Lauren  and
      Lin, Jessica  and
      Liu, {Yang Janet}  and
      Peng, Siyao  and
      Zhu, Yilun  and
      Zeldes, Amir",
    editor = "Prange, Jakob  and
      Friedrich, Annemarie",
    booktitle = "Proceedings of the 17th Linguistic Annotation Workshop (LAW-XVII)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.law-1.17",
    doi = "10.18653/v1/2023.law-1.17",
    pages = "166--178",
    abstract = "We present GENTLE, a new mixed-genre English challenge corpus totaling 17K tokens and consisting of 8 unusual text types for out-of-domain evaluation: dictionary entries, esports commentaries, legal documents, medical notes, poetry, mathematical proofs, syllabuses, and threat letters. GENTLE is manually annotated for a variety of popular NLP tasks, including syntactic dependency parsing, entity recognition, coreference resolution, and discourse parsing. We evaluate state-of-the-art NLP systems on GENTLE and find severe degradation for at least some genres in their performance on all tasks, which indicates GENTLE{'}s utility as an evaluation dataset for NLP systems.",
    keywords = {workshop},
}

@inproceedings{gessler-2022-closing,
    title = "Closing the {NLP} Gap: Documentary Linguistics and {NLP} Need a Shared Software Infrastructure",
    author = "Gessler, Luke",
    editor = "Moeller, Sarah  and
      Anastasopoulos, Antonios  and
      Arppe, Antti  and
      Chaudhary, Aditi  and
      Harrigan, Atticus  and
      Holden, Josh  and
      Lachler, Jordan  and
      Palmer, Alexis  and
      Rijhwani, Shruti  and
      Schwartz, Lane",
    booktitle = "Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.computel-1.15",
    doi = "10.18653/v1/2022.computel-1.15",
    pages = "119--126",
    abstract = "For decades, researchers in natural language processing and computational linguistics have been developing models and algorithms that aim to serve the needs of language documentation projects. However, these models have seen little use in language documentation despite their great potential for making documentary linguistic artefacts better and easier to produce. In this work, we argue that a major reason for this NLP gap is the lack of a strong foundation of application software which can on the one hand serve the complex needs of language documentation and on the other hand provide effortless integration with NLP models. We further present and describe a work-in-progress system we have developed to serve this need, Glam.",
    keywords = {workshop},
}

@inproceedings{gessler-etal-2022-xposition,
    title = "Xposition: An Online Multilingual Database of Adpositional Semantics",
    author = "Gessler, Luke  and
      Blodgett, Austin  and
      Ledford, Joseph C.  and
      Schneider, Nathan",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.194",
    pages = "1824--1830",
    abstract = "We present Xposition, an online platform for documenting adpositional semantics across languages in terms of supersenses (Schneider et al., 2018). More than just a lexical database, Xposition houses annotation guidelines, structured lexicographic documentation, and annotated corpora. Guidelines and documentation are stored as wiki pages for ease of editing, and described elements (supersenses, adpositions, etc.) are hyperlinked for ease of browsing. We describe how the platform structures information; its current contents across several languages; and aspects of the design of the web application that supports it, with special attention to how it supports datasets and standards that evolve over time.",
    keywords = {workshop},
}

@inproceedings{gessler-zeldes-2022-microbert,
    title = "{M}icro{BERT}: Effective Training of Low-resource Monolingual {BERT}s through Parameter Reduction and Multitask Learning",
    author = "Gessler, Luke  and
      Zeldes, Amir",
    editor = {Ataman, Duygu  and
      Gonen, Hila  and
      Ruder, Sebastian  and
      Firat, Orhan  and
      G{\"u}l Sahin, G{\"o}zde  and
      Mirzakhalov, Jamshidbek},
    booktitle = "Proceedings of the 2nd Workshop on Multi-lingual Representation Learning (MRL)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.mrl-1.9",
    doi = "10.18653/v1/2022.mrl-1.9",
    pages = "86--99",
    abstract = "BERT-style contextualized word embedding models are critical for good performance in most NLP tasks, but they are data-hungry and therefore difficult to train for low-resource languages. In this work, we investigate whether a combination of greatly reduced model size and two linguistically rich auxiliary pretraining tasks (part-of-speech tagging and dependency parsing) can help produce better BERTs in a low-resource setting. Results from 7 diverse languages indicate that our model, MicroBERT, is able to produce marked improvements in downstream task evaluations, including gains up to 18{\%} for parser LAS and 11{\%} for NER F1 compared to an mBERT baseline, and we achieve these results with less than 1{\%} of the parameter count of a multilingual BERT base{--}sized model. We conclude that training very small BERTs and leveraging any available labeled data for multitask learning during pretraining can produce models which outperform both their multilingual counterparts and traditional fixed embeddings for low-resource languages.",
    keywords = {workshop},
}

@inproceedings{gessler-etal-2022-midas,
    title = "{M}idas Loop: A Prioritized Human-in-the-Loop Annotation for Large Scale Multilayer Data",
    author = "Gessler, Luke  and
      Levine, Lauren  and
      Zeldes, Amir",
    editor = "Pradhan, Sameer  and
      Kuebler, Sandra",
    booktitle = "Proceedings of the 16th Linguistic Annotation Workshop (LAW-XVI) within LREC2022",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.law-1.13",
    pages = "103--110",
    abstract = "Large scale annotation of rich multilayer corpus data is expensive and time consuming, motivating approaches that integrate high quality automatic tools with active learning in order to prioritize human labeling of hard cases. A related challenge in such scenarios is the concurrent management of automatically annotated data and human annotated data, particularly where different subsets of the data have been corrected for different types of annotation and with different levels of confidence. In this paper we present [REDACTED], a collaborative, version-controlled online annotation environment for multilayer corpus data which includes integrated provenance and confidence metadata for each piece of information at the document, sentence, token and annotation level. We present a case study on improving annotation quality in an existing multilayer parse bank of English called AMALGUM, focusing on active learning in corpus preprocessing, at the surprisingly challenging level of sentence segmentation. Our results show improvements to state-of-the-art sentence segmentation and a promising workflow for getting {``}silver{''} data to approach gold standard quality.",
    keywords = {workshop},
}

@inproceedings{gessler-etal-2021-discodisco,
    title = "{D}is{C}o{D}is{C}o at the {DISRPT}2021 Shared Task: A System for Discourse Segmentation, Classification, and Connective Detection",
    author = "Gessler, Luke  and
      Behzad, Shabnam  and
      Liu, {Yang Janet}  and
      Peng, Siyao  and
      Zhu, Yilun  and
      Zeldes, Amir",
    editor = "Zeldes, Amir  and
      Liu, {Yang Janet}  and
      Iruskieta, Mikel  and
      Muller, Philippe  and
      Braud, Chlo{\'e}  and
      Badene, Sonia",
    booktitle = "Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021)",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.disrpt-1.6",
    doi = "10.18653/v1/2021.disrpt-1.6",
    pages = "51--62",
    abstract = "This paper describes our submission to the DISRPT2021 Shared Task on Discourse Unit Segmentation, Connective Detection, and Relation Classification. Our system, called DisCoDisCo, is a Transformer-based neural classifier which enhances contextualized word embeddings (CWEs) with hand-crafted features, relying on tokenwise sequence tagging for discourse segmentation and connective detection, and a feature-rich, encoder-less sentence pair classifier for relation classification. Our results for the first two tasks outperform SOTA scores from the previous 2019 shared task, and results on relation classification suggest strong performance on the new 2021 benchmark. Ablation tests show that including features beyond CWEs are helpful for both tasks, and a partial evaluation of multiple pretrained Transformer-based language models indicates that models pre-trained on the Next Sentence Prediction (NSP) task are optimal for relation classification.",
    keywords = {workshop},
}

@inproceedings{gessler-schneider-2021-bert,
    title = "{BERT} Has Uncommon Sense: Similarity Ranking for Word Sense {BERT}ology",
    author = "Gessler, Luke  and
      Schneider, Nathan",
    editor = "Bastings, Jasmijn  and
      Belinkov, Yonatan  and
      Dupoux, Emmanuel  and
      Giulianelli, Mario  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.blackboxnlp-1.43",
    doi = "10.18653/v1/2021.blackboxnlp-1.43",
    pages = "539--547",
    abstract = "An important question concerning contextualized word embedding (CWE) models like BERT is how well they can represent different word senses, especially those in the long tail of uncommon senses. Rather than build a WSD system as in previous work, we investigate contextualized embedding neighborhoods directly, formulating a query-by-example nearest neighbor retrieval task and examining ranking performance for words and senses in different frequency bands. In an evaluation on two English sense-annotated corpora, we find that several popular CWE models all outperform a random baseline even for proportionally rare senses, without explicit sense supervision. However, performance varies considerably even among models with similar architectures and pretraining regimes, with especially large differences for rare word senses, revealing that CWE models are not all created equal when it comes to approximating word senses in their native representations.",
    keywords = {workshop},
}

@inproceedings{gessler-etal-2020-amalgum,
    title = "{AMALGUM} {--} A Free, Balanced, Multilayer {E}nglish Web Corpus",
    author = "Gessler, Luke  and
      Peng, Siyao  and
      Liu, {Yang Janet}  and
      Zhu, Yilun  and
      Behzad, Shabnam  and
      Zeldes, Amir",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.648",
    pages = "5267--5275",
    abstract = "We present a freely available, genre-balanced English web corpus totaling 4M tokens and featuring a large number of high-quality automatic annotation layers, including dependency trees, non-named entity annotations, coreference resolution, and discourse trees in Rhetorical Structure Theory. By tapping open online data sources the corpus is meant to offer a more sizable alternative to smaller manually created annotated data sets, while avoiding pitfalls such as imbalanced or unknown composition, licensing problems, and low-quality natural language processing. We harness knowledge from multiple annotation layers in order to achieve a {``}better than NLP{''} benchmark and evaluate the accuracy of the resulting resource.",
    language = "English",
    ISBN = "979-10-95546-34-4",
    keywords = {conf},
}

@inproceedings{arora-etal-2020-supervised,
    title = "Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in {H}indi and {P}unjabi",
    author = "Arora, Aryaman  and
      Gessler, Luke  and
      Schneider, Nathan",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.696",
    doi = "10.18653/v1/2020.acl-main.696",
    pages = "7791--7795",
    abstract = "Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches. We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries. Our best Hindi model achieves state of the art performance, and also achieves good performance on a closely related language, Punjabi, without modification.",
    keywords = {conf},
}

@inproceedings{gessler-etal-2020-supersense,
    title = "Supersense and Sensibility: Proxy Tasks for Semantic Annotation of Prepositions",
    author = "Gessler, Luke  and
      Wein, Shira  and
      Schneider, Nathan",
    editor = "Dipper, Stefanie  and
      Zeldes, Amir",
    booktitle = "Proceedings of the 14th Linguistic Annotation Workshop",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.law-1.11",
    pages = "117--126",
    abstract = "Prepositional supersense annotation is time-consuming and requires expert training. Here, we present two sensible methods for obtaining prepositional supersense annotations indirectly by eliciting surface substitution and similarity judgments. Four pilot studies suggest that both methods have potential for producing prepositional supersense annotations that are comparable in quality to expert annotations.",
    keywords = {workshop},
}

@inproceedings{neubig-etal-2020-summary,
    title = "A Summary of the First Workshop on Language Technology for Language Documentation and Revitalization",
    author = "Neubig, Graham  and
      Rijhwani, Shruti  and
      Palmer, Alexis  and
      MacKenzie, Jordan  and
      Cruz, Hilaria  and
      Li, Xinjian  and
      Lee, Matthew  and
      Chaudhary, Aditi  and
      Gessler, Luke  and
      Abney, Steven  and
      Hayati, Shirley Anugrah  and
      Anastasopoulos, Antonios  and
      Zamaraeva, Olga  and
      Prud{'}hommeaux, Emily  and
      Child, Jennette  and
      Child, Sara  and
      Knowles, Rebecca  and
      Moeller, Sarah  and
      Micher, Jeffrey  and
      Li, Yiyuan  and
      Zink, Sydney  and
      Xia, Mengzhou  and
      Sharma, Roshan  and
      Littell, Patrick",
    editor = "Beermann, Dorothee  and
      Besacier, Laurent  and
      Sakti, Sakriani  and
      Soria, Claudia",
    booktitle = "Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources association",
    url = "https://aclanthology.org/2020.sltu-1.48",
    pages = "342--351",
    abstract = "Despite recent advances in natural language processing and other language technology, the application of such technology to language documentation and conservation has been limited. In August 2019, a workshop was held at Carnegie Mellon University in Pittsburgh, PA, USA to attempt to bring together language community members, documentary linguists, and technologists to discuss how to bridge this gap and create prototypes of novel and practical language revitalization technologies. The workshop focused on developing technologies to aid language documentation and revitalization in four areas: 1) spoken language (speech transcription, phone to orthography decoding, text-to-speech and text-speech forced alignment), 2) dictionary extraction and management, 3) search tools for corpora, and 4) social media (language learning bots and social media analysis). This paper reports the results of this workshop, including issues discussed, and various conceived and implemented technologies for nine languages: Arapaho, Cayuga, Inuktitut, Irish Gaelic, Kidaw{'}ida, Kwak{'}wala, Ojibwe, San Juan Quiahije Chatino, and Seneca.",
    language = "English",
    ISBN = "979-10-95546-35-1",
    keywords = {workshop},
}

@inproceedings{gessler-etal-2019-discourse,
    title = "A Discourse Signal Annotation System for {RST} Trees",
    author = "Gessler, Luke  and
      Liu, {Yang Janet}  and
      Zeldes, Amir",
    editor = "Zeldes, Amir  and
      Das, Debopam  and
      Galani, Erick Maziero  and
      Antonio, Juliano Desiderato  and
      Iruskieta, Mikel",
    booktitle = "Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019",
    month = jun,
    year = "2019",
    address = "Minneapolis, MN",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-2708",
    doi = "10.18653/v1/W19-2708",
    pages = "56--61",
    abstract = "This paper presents a new system for open-ended discourse relation signal annotation in the framework of Rhetorical Structure Theory (RST), implemented on top of an online tool for RST annotation. We discuss existing projects annotating textual signals of discourse relations, which have so far not allowed simultaneously structuring and annotating words signaling hierarchical discourse trees, and demonstrate the design and applications of our interface by extending existing RST annotations in the freely available GUM corpus.",
    keywords = {workshop},
}

@inproceedings{abrams-etal-2019-b,
    title = "{B}. Rex: a dialogue agent for book recommendations",
    author = "Abrams, Mitchell  and
      Gessler, Luke  and
      Marge, Matthew",
    editor = "Nakamura, Satoshi  and
      Gasic, Milica  and
      Zukerman, Ingrid  and
      Skantze, Gabriel  and
      Nakano, Mikio  and
      Papangelis, Alexandros  and
      Ultes, Stefan  and
      Yoshino, Koichiro",
    booktitle = "Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue",
    month = sep,
    year = "2019",
    address = "Stockholm, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5948",
    doi = "10.18653/v1/W19-5948",
    pages = "418--421",
    abstract = "We present B. Rex, a dialogue agent for book recommendations. B. Rex aims to exploit the cognitive ease of natural dialogue and the excitement of a whimsical persona in order to engage users who might not enjoy using more common interfaces for finding new books. B. Rex succeeds in making book recommendations with good quality based on only information revealed by the user in the dialogue.",
    keywords = {conf},
}

@inproceedings{gessler-2019-developing,
    title = "Developing without developers: choosing labor-saving tools for language documentation apps",
    author = "Gessler, Luke",
    editor = "Arppe, Antti  and
      Good, Jeff  and
      Hulden, Mans  and
      Lachler, Jordan  and
      Palmer, Alexis  and
      Schwartz, Lane  and
      Silfverberg, Miikka",
    booktitle = "Proceedings of the 3rd Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers)",
    month = feb,
    year = "2019",
    address = "Honolulu",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-6002",
    pages = "6--13",
    keywords = {workshop},
}