<!doctype html>
<html lang="en">
<head>
<title>Luke D. Gessler</title>
<!-- 2024-08-26 Mon 18:36 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="generator" content="Org-mode">
<meta name="author" content="Luke D. Gessler">

<link  href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.5/js/bootstrap.min.js"></script>
<style>
/* org mode styles on top of twbs */

html {
    position: relative;
    min-height: 100%;
}

body {
    font-size: 18px;
    margin-bottom: 105px;
}

footer {
    position: absolute;
    bottom: 0;
    width: 100%;
    height: 101px;
    background-color: #f5f5f5;
}

footer > div {
    padding: 10px;
}

footer p {
    margin: 0 0 5px;
    text-align: center;
    font-size: 16px;
}

#table-of-contents {
    margin-top: 20px;
    margin-bottom: 20px;
}

blockquote p {
    font-size: 18px;
}

pre {
    font-size: 16px;
}

.footpara {
    display: inline-block;
}

figcaption {
  font-size: 16px;
  color: #666;
  font-style: italic;
  padding-bottom: 15px;
}

/* from twbs docs */

.bs-docs-sidebar.affix {
    position: static;
}
@media (min-width: 768px) {
    .bs-docs-sidebar {
        padding-left: 20px;
    }
}

/* All levels of nav */
.bs-docs-sidebar .nav > li > a {
    display: block;
    padding: 4px 20px;
    font-size: 14px;
    font-weight: 500;
    color: #999;
}
.bs-docs-sidebar .nav > li > a:hover,
.bs-docs-sidebar .nav > li > a:focus {
    padding-left: 19px;
    color: #A1283B;
    text-decoration: none;
    background-color: transparent;
    border-left: 1px solid #A1283B;
}
.bs-docs-sidebar .nav > .active > a,
.bs-docs-sidebar .nav > .active:hover > a,
.bs-docs-sidebar .nav > .active:focus > a {
    padding-left: 18px;
    font-weight: bold;
    color: #A1283B;
    background-color: transparent;
    border-left: 2px solid #A1283B;
}

/* Nav: second level (shown on .active) */
.bs-docs-sidebar .nav .nav {
    display: none; /* Hide by default, but at >768px, show it */
    padding-bottom: 10px;
}
.bs-docs-sidebar .nav .nav > li > a {
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 30px;
    font-size: 12px;
    font-weight: normal;
}
.bs-docs-sidebar .nav .nav > li > a:hover,
.bs-docs-sidebar .nav .nav > li > a:focus {
    padding-left: 29px;
}
.bs-docs-sidebar .nav .nav > .active > a,
.bs-docs-sidebar .nav .nav > .active:hover > a,
.bs-docs-sidebar .nav .nav > .active:focus > a {
    padding-left: 28px;
    font-weight: 500;
}

/* Nav: third level (shown on .active) */
.bs-docs-sidebar .nav .nav .nav {
    padding-bottom: 10px;
}
.bs-docs-sidebar .nav .nav .nav > li > a {
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 40px;
    font-size: 12px;
    font-weight: normal;
}
.bs-docs-sidebar .nav .nav .nav > li > a:hover,
.bs-docs-sidebar .nav .nav .nav > li > a:focus {
    padding-left: 39px;
}
.bs-docs-sidebar .nav .nav .nav > .active > a,
.bs-docs-sidebar .nav .nav .nav > .active:hover > a,
.bs-docs-sidebar .nav .nav .nav > .active:focus > a {
    padding-left: 38px;
    font-weight: 500;
}

/* Show and affix the side nav when space allows it */
@media (min-width: 992px) {
    .bs-docs-sidebar .nav > .active > ul {
        display: block;
    }
    /* Widen the fixed sidebar */
    .bs-docs-sidebar.affix,
    .bs-docs-sidebar.affix-bottom {
        width: 213px;
    }
    .bs-docs-sidebar.affix {
        position: fixed; /* Undo the static from mobile first approach */
        top: 20px;
    }
    .bs-docs-sidebar.affix-bottom {
        position: absolute; /* Undo the static from mobile first approach */
    }
    .bs-docs-sidebar.affix .bs-docs-sidenav,.bs-docs-sidebar.affix-bottom .bs-docs-sidenav {
        margin-top: 0;
        margin-bottom: 0
    }
}
@media (min-width: 1200px) {
    /* Widen the fixed sidebar again */
    .bs-docs-sidebar.affix-bottom,
    .bs-docs-sidebar.affix {
        width: 263px;
    }
}
</style>
<script>
$(function() {
    'use strict';

    $('.bs-docs-sidebar li').first().addClass('active');

    $(document.body).scrollspy({target: '.bs-docs-sidebar'});

    $('.bs-docs-sidebar').affix();
});
</script><link rel='stylesheet' type='text/css' href='/css/styles.css'>
            <link href="https://fonts.googleapis.com/css?family=Roboto"
                  rel="stylesheet">
            <script type="text/javascript" src="https://cdn.rawgit.com/pcooksey/bibtex-js/ef59e62c/src/bibtex_js.js"></script>
</head>
<body>
<div id="content" class="container">
<div class="row"><div class="col-md-12"><h1 class="title">Luke D. Gessler</h1>
<img src="img/luke.jpeg" style="float:right;border-radius:50%;width:240px;height:240px;">

<p>
Hi, I'm Luke!
I'm an assistant professor at <a href="https://linguistics.indiana.edu/index.html">Indiana University's Department of Linguistics</a>.
Previously, I was a postdoc in the <a href="https://nala-cub.github.io/">NALA Group</a> at CU Boulder, and I got my Ph.D. in computational linguistics at <a href="https://linguistics.georgetown.edu">Georgetown University</a> with the <a href="https://gucorpling.org/corpling/">Corpling lab</a> and <a href="https://nert.georgetown.edu/">NERT</a>.
</p>

<p>
I am interested in low-resource NLP and language resource development, particularly in the context of endangered language documentation.
Linguists are <a href="https://specgram.com/CLXXV.1/03.carlson.cartoon5.html">often asked</a> how many languages they speak, so let me share that mine include English (native), Latin (reading), Hindi-Urdu (conversational), Sahidic Coptic (reading), and bits and pieces of others.
</p>

<div id="outline-container-sec-" class="outline-2">
<h2 id="sec-">Research</h2>
<div class="outline-text-2" id="text-">
<p>
Modern methods in natural language processing (NLP) from the past 10 years are powerful but require great amounts of data.
This has led to a performance gap between a handful of high-resource languages which have enough data to fully exploit models' capabilities (like Russian or English), and low-resource languages (like Mohawk or Uyghur), which lack the data volume required to fully realize models' potential.
This is a regrettable circumstance, as poorer performing models may induce many inequities (such as socioeconomic ones), and most of the world's languages may not be spoken by the close of the century and are in need of high-quality language technologies to aid efforts to document and revitalize them.
</p>

<p>
My work is broadly aimed at addressing this performance gap between high- and low-resource languages by developing linguistically-sophisticated resources and algorithms which can help low-resource languages overcome the onerous demands of the deep learning methods which have become dominant in NLP.
Most of my work can be seen as belonging to at least one of the three following threads:
</p>

<ul class="org-ul">
<li>Language resource development: creation and maintenance of natural language corpora enriched with linguistic analyses.
</li>
<li>NLP-capable language documentation systems: developing systems aimed at language documentation, i.e. the process of collecting and describing data in a particular language, with an emphasis on deep integration with NLP systems to facilitate the documentary process.
</li>
<li>Low-resource NLP: development of methods specifically for languages with little data.
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-" class="outline-2">
<h2 id="sec-">Online</h2>
<div class="outline-text-2" id="text-">
<p>
Here is my&#x2026;
</p>
<ul class="org-ul">
<li>Email: lukegessler@gmail.com
</li>
<li><a href="./cv.pdf">CV</a>
</li>
<li><a href="https://github.com/lgessler">GitHub</a>
</li>
<li><a href="https://twitter.com/LukeGessler">Twitter</a>
</li>
<li><a href="https://aclanthology.org/people/l/luke-gessler/">ACL Anthology page</a>
</li>
<li><a href="https://scholar.google.com/citations?user=ppYCkqgAAAAJ&amp;hl=en">Google Scholar page</a>
</li>
<li><a href="https://news.ycombinator.com/user?id=lgessler">Hacker News account</a>
</li>
</ul>

<p>
Also, I maintain the <a href="./mala/">Map of Applications for Linguistic Annotation</a>.
</p>
</div>
</div>

<div id="outline-container-sec-" class="outline-2">
<h2 id="sec-">Recent work</h2>
<div class="outline-text-2" id="text-">
<div class="bibtex_template">
  <div class="entry">
	<div class="autolist">
	  <div class="if author" style="margin-top:5px;   padding-left: 20px; text-indent: -20px;">
		<span class="author" style="font-weight: bold; "></span>
		<span class="if year">
		  (<span class="year"></span>),
		</span>
		<span class="if journal">"</span><span class="if booktitle">"</span><span class="if school"><span class="title" style="font-style: italic"></span></span><span class="if !school"><span class="title"></span></span><span class="if journal">".</span><span class="if booktitle">". </span><span class="if school">. </span>
		<span class="if journal" style="">
		  <span class="journal" style="font-style:italic"></span>
		  <span class="if volume">
			<span class="volume"></span>
			<span class="if number">
			  (<span class="number"></span>)</span><span class="if pages">, <span class="pages"></span>.
			</span>
		  </span>
		</span>
		<span class="if booktitle" style="">In:
		  <span class="booktitle" style="font-style:italic"></span>.<span class="if series" style=""> (<span class="series" style=""></span>.)</span>
		  <span class="if publisher">
			<span class="publisher"></span>:
		  </span>
		  <span class="if address">
			<span class="address"></span></span><span class="if pages">, <span class="pages"></span></span>.
		</span>
		<span class="if school" style="">
		  <span class="if type"><span class="type"></span>,</span>
		  <span class="school" style=""></span>.
		</span>
		<span class="if url">
		  <br/>[<a class="bibtexVar" href="+url+" style="color:black; font-size:10px" extra="url">
			<span class="if comment">
			  <span class="comment"></span>
			</span>
			<span class="if !comment">
			  <span>link</span>
			</span>
		  </a>]
		</span>
		<span class="if url2">
		  [<a class="bibtexVar" href="+url2+" style="color:black; font-size:10px" extra="url2">
			<span class="if comment2">
			  <span class="comment2"></span>
			</span>
			<span class="if !comment2">
			  <span>link</span>
			</span>
		  </a>]
		</span>
		<span class="if doi">
		  <span class="if !url"><br/></span>[<a class="bibtexVar" href="+doi+" style="color:black; font-size:10px" extra="doi">DOI</a>]
		</span>
	  </div>
	</div>
  </div>
</div>
<div id="bibtex_display"></div>
<textarea id="bibtex_input" style="display:none;">
@inproceedings{gessler-2024-pronto,
    title = "{P}r{O}nto: Language Model Evaluations for 859 Languages",
    author = "Gessler, Luke",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1159",
    pages = "13243--13256",
    abstract = "Evaluation datasets are critical resources for measuring the quality of pretrained language models. However, due to the high cost of dataset annotation, these resources are scarce for most languages other than English, making it difficult to assess the quality of language models. In this work, we present a new method for evaluation dataset construction which enables any language with a New Testament translation to receive a suite of evaluation datasets suitable for pretrained language model evaluation. The method critically involves aligning verses with those in the New Testament portion of English OntoNotes, and then projecting annotations from English to the target language, with no manual annotation required. We apply this method to 1051 New Testament translations in 859 languages and make them publicly available. Additionally, we conduct experiments which demonstrate the efficacy of our method for creating evaluation tasks which can assess language model quality.",
}

@inproceedings{gessler-von-der-wense-2024-nlp,
    title = "{NLP} for Language Documentation: Two Reasons for the Gap between Theory and Practice",
    author = "Gessler, Luke  and
      von der Wense, Katharina",
    editor = "Mager, Manuel  and
      Ebrahimi, Abteen  and
      Rijhwani, Shruti  and
      Oncevay, Arturo  and
      Chiruzzo, Luis  and
      Pugh, Robert  and
      von der Wense, Katharina",
    booktitle = "Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.americasnlp-1.1",
    doi = "10.18653/v1/2024.americasnlp-1.1",
    pages = "1--6",
    abstract = "Both NLP researchers and linguists have expressed a desire to use language technologies in language documentation, but most documentary work still proceeds without them, presenting a lost opportunity to hasten the preservation of the world{'}s endangered languages, such as those spoken in Latin America. In this work, we empirically measure two factors that have previously been identified as explanations of this low utilization: curricular offerings in graduate programs, and rates of interdisciplinary collaboration in publications related to NLP in language documentation. Our findings verify the claim that interdisciplinary training and collaborations are scarce and support the view that interdisciplinary curricular offerings facilitate interdisciplinary collaborations.",
}

@inproceedings{rice-etal-2024-tams,
    title = "{TAMS}: Translation-Assisted Morphological Segmentation",
    author = "Rice, Enora  and
      Marashian, Ali  and
      Gessler, Luke  and
      Palmer, Alexis  and
      Wense, Katharina",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.366",
    pages = "6752--6765",
    abstract = "Canonical morphological segmentation is the process of analyzing words into the standard (\textit{aka} underlying) forms of their constituent morphemes.This is a core task in endangered language documentation, and NLP systems have the potential to dramatically speed up this process. In typical language documentation settings, training data for canonical morpheme segmentation is scarce, making it difficult to train high quality models. However, translation data is often much more abundant, and, in this work, we present a method that attempts to leverage translation data in the canonical segmentation task. We propose a character-level sequence-to-sequence model that incorporates representations of translations obtained from pretrained high-resource monolingual language models as an additional signal. Our model outperforms the baseline in a super-low resource setting but yields mixed results on training splits with more data. Additionally, we find that we can achieve strong performance even without needing difficult-to-obtain word level alignments. While further work is needed to make translations useful in higher-resource settings, our model shows promise in severely resource-constrained settings.",
}

@inproceedings{gessler-schneider-2023-syntactic,
    title = "Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?",
    author = "Gessler, Luke  and
      Schneider, Nathan",
    editor = "Jiang, Jing  and
      Reitter, David  and
      Deng, Shumin",
    booktitle = "Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-1.17",
    doi = "10.18653/v1/2023.conll-1.17",
    pages = "238--253",
    abstract = "A line of work on Transformer-based language models such as BERT has attempted to use syntactic inductive bias to enhance the pretraining process, on the theory that building syntactic structure into the training process should reduce the amount of data needed for training. But such methods are often tested for high-resource languages such as English. In this work, we investigate whether these methods can compensate for data sparseness in low-resource languages, hypothesizing that they ought to be more effective for low-resource languages. We experiment with five low-resource languages: Uyghur, Wolof, Maltese, Coptic, and Ancient Greek. We find that these syntactic inductive bias methods produce uneven results in low-resource settings, and provide surprisingly little benefit in most cases.",
}

@inproceedings{aoyama-etal-2023-gentle,
    title = "{GENTLE}: A Genre-Diverse Multilayer Challenge Set for {E}nglish {NLP} and Linguistic Evaluation",
    author = "Aoyama, Tatsuya  and
      Behzad, Shabnam  and
      Gessler, Luke  and
      Levine, Lauren  and
      Lin, Jessica  and
      Liu, Yang Janet  and
      Peng, Siyao  and
      Zhu, Yilun  and
      Zeldes, Amir",
    editor = "Prange, Jakob  and
      Friedrich, Annemarie",
    booktitle = "Proceedings of the 17th Linguistic Annotation Workshop (LAW-XVII)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.law-1.17",
    doi = "10.18653/v1/2023.law-1.17",
    pages = "166--178",
    abstract = "We present GENTLE, a new mixed-genre English challenge corpus totaling 17K tokens and consisting of 8 unusual text types for out-of-domain evaluation: dictionary entries, esports commentaries, legal documents, medical notes, poetry, mathematical proofs, syllabuses, and threat letters. GENTLE is manually annotated for a variety of popular NLP tasks, including syntactic dependency parsing, entity recognition, coreference resolution, and discourse parsing. We evaluate state-of-the-art NLP systems on GENTLE and find severe degradation for at least some genres in their performance on all tasks, which indicates GENTLE{'}s utility as an evaluation dataset for NLP systems.",
}

@inproceedings{gessler-zeldes-2022-microbert,
    title = "{M}icro{BERT}: Effective Training of Low-resource Monolingual {BERT}s through Parameter Reduction and Multitask Learning",
    author = "Gessler, Luke  and
      Zeldes, Amir",
    booktitle = "Proceedings of the The 2nd Workshop on Multi-lingual Representation Learning (MRL)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.mrl-1.9",
    pages = "86--99",
    abstract = "BERT-style contextualized word embedding models are critical for good performance in most NLP tasks, but they are data-hungry and therefore difficult to train for low-resource languages. In this work, we investigate whether a combination of greatly reduced model size and two linguistically rich auxiliary pretraining tasks (part-of-speech tagging and dependency parsing) can help produce better BERTs in a low-resource setting. Results from 7 diverse languages indicate that our model, MicroBERT, is able to produce marked improvements in downstream task evaluations, including gains up to 18{\%} for parser LAS and 11{\%} for NER F1 compared to an mBERT baseline, and we achieve these results with less than 1{\%} of the parameter count of a multilingual BERT base{--}sized model. We conclude that training very small BERTs and leveraging any available labeled data for multitask learning during pretraining can produce models which outperform both their multilingual counterparts and traditional fixed embeddings for low-resource languages.",
}

@inproceedings{gessler-2022-closing,
    title = "Closing the {NLP} Gap: Documentary Linguistics and {NLP} Need a Shared Software Infrastructure",
    author = "Gessler, Luke",
    booktitle = "Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.computel-1.15",
    doi = "10.18653/v1/2022.computel-1.15",
    pages = "119--126",
    abstract = "For decades, researchers in natural language processing and computational linguistics have been developing models and algorithms that aim to serve the needs of language documentation projects. However, these models have seen little use in language documentation despite their great potential for making documentary linguistic artefacts better and easier to produce. In this work, we argue that a major reason for this NLP gap is the lack of a strong foundation of application software which can on the one hand serve the complex needs of language documentation and on the other hand provide effortless integration with NLP models. We further present and describe a work-in-progress system we have developed to serve this need, Glam.",
}

@inproceedings{gessler_midas_2022,
	address = {Marseille, France},
	title = {Midas {Loop}: {A} {Prioritized} {Human}-in-the-{Loop} {Annotation} for {Large} {Scale} {Multilayer} {Data}},
	url = {https://aclanthology.org/2022.lawxvi-1.13},
	abstract = {Large scale annotation of rich multilayer corpus data is expensive and time consuming, motivating approaches that integrate high quality automatic tools with active learning in order to prioritize human labeling of hard cases. A related challenge in such scenarios is the concurrent management of automatically annotated data and human annotated data, particularly where different subsets of the data have been corrected for different types of annotation and with different levels of confidence. In this paper we present [REDACTED], a collaborative, version-controlled online annotation environment for multilayer corpus data which includes integrated provenance and confidence metadata for each piece of information at the document, sentence, token and annotation level. We present a case study on improving annotation quality in an existing multilayer parse bank of English called AMALGUM, focusing on active learning in corpus preprocessing, at the surprisingly challenging level of sentence segmentation. Our results show improvements to state-of-the-art sentence segmentation and a promising workflow for getting "silver" data to approach gold standard quality.},
	booktitle = {Proceedings of {The} 16th {Lingusitic} {Annotation} {Workshop} ({LAW}-{XVI}) within {LREC2022}},
	publisher = {European Language Resources Association},
	author = {Gessler, Luke and Levine, Lauren and Zeldes, Amir},
	month = jun,
	year = {2022},
	pages = {103--110},
}

@inproceedings{gessler_xposition_2022,
	address = {Marseille, France},
	title = {Xposition: {An} {Online} {Multilingual} {Database} of {Adpositional} {Semantics}},
	url = {https://aclanthology.org/2022.lrec-1.194},
	abstract = {We present Xposition, an online platform for documenting adpositional semantics across languages in terms of supersenses (Schneider et al., 2018). More than just a lexical database, Xposition houses annotation guidelines, structured lexicographic documentation, and annotated corpora. Guidelines and documentation are stored as wiki pages for ease of editing, and described elements (supersenses, adpositions, etc.) are hyperlinked for ease of browsing. We describe how the platform structures information; its current contents across several languages; and aspects of the design of the web application that supports it, with special attention to how it supports datasets and standards that evolve over time.},
	booktitle = {Proceedings of the {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Gessler, Luke and Schneider, Nathan and Ledford, Joseph C. and Blodgett, Austin},
	month = jun,
	year = {2022},
	pages = {1824--1830},
}

@inproceedings{uncommon-sense,
    title = "{BERT} has uncommon sense: similarity ranking for word sense {BERTology}",
    author = "Gessler, Luke  and
      Schneider, Nathan",
    booktitle = "Proc. of BlackboxNLP at EMNLP",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2109.09780",
}

@inproceedings{discodisco,
    title = "{DisCoDisCo} at the {DISRPT2021} shared task: a system for discourse segmentation, classification, and connective detection",
    author = "Gessler, Luke  and
      Behzad, Shabnam  and
      Liu, Yang Janet  and
      Peng, Siyao  and
      Zhu, Yilun  and
      Zeldes, Amir",
    booktitle = "Proc. of CODI-DISRPT at EMNLP",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2109.09777",
}

@inproceedings{gessler-etal-2020-supersense,
    title = "Supersense and Sensibility: Proxy Tasks for Semantic Annotation of Prepositions",
    author = "Gessler, Luke  and
      Wein, Shira  and
      Schneider, Nathan",
    booktitle = "Proceedings of LAW at ACL",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.law-1.11",
    pages = "117--126",
    abstract = "Prepositional supersense annotation is time-consuming and requires expert training. Here, we present two sensible methods for obtaining prepositional supersense annotations indirectly by eliciting surface substitution and similarity judgments. Four pilot studies suggest that both methods have potential for producing prepositional supersense annotations that are comparable in quality to expert annotations.",
}

@inproceedings{gessler-etal-2020-amalgum,
    title = "{AMALGUM} {--} A Free, Balanced, Multilayer {E}nglish Web Corpus",
    author = "Gessler, Luke  and
      Peng, Siyao  and
      Liu, Yang  and
      Zhu, Yilun  and
      Behzad, Shabnam  and
      Zeldes, Amir",
    booktitle = "Proc. of LREC",
    month = may,
    year = "2020",
    address = "Online",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.648",
    pages = "5267--5275",
    abstract = "We present a freely available, genre-balanced English web corpus totaling 4M tokens and featuring a large number of high-quality automatic annotation layers, including dependency trees, non-named entity annotations, coreference resolution, and discourse trees in Rhetorical Structure Theory. By tapping open online data sources the corpus is meant to offer a more sizable alternative to smaller manually created annotated data sets, while avoiding pitfalls such as imbalanced or unknown composition, licensing problems, and low-quality natural language processing. We harness knowledge from multiple annotation layers in order to achieve a {``}better than NLP{''} benchmark and evaluate the accuracy of the resulting resource.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{arora-etal-2020-supervised,
    title = "Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in {H}indi and {P}unjabi",
    author = "Arora, Aryaman  and
      Gessler, Luke  and
      Schneider, Nathan",
    booktitle = "Proc. of ACL",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.696",
    doi = "10.18653/v1/2020.acl-main.696",
    pages = "7791--7795",
    abstract = "Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches. We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries. Our best Hindi model achieves state of the art performance, and also achieves good performance on a closely related language, Punjabi, without modification.",
}

@inproceedings{neubig-etal-2020-summary,
    title = "A Summary of the First Workshop on Language Technology for Language Documentation and Revitalization",
    author = "Neubig, Graham  and
      Rijhwani, Shruti  and
      Palmer, Alexis  and
      MacKenzie, Jordan  and
      Cruz, Hilaria  and
      Li, Xinjian  and
      Lee, Matthew  and
      Chaudhary, Aditi  and
      Gessler, Luke  and
      Abney, Steven  and
      Hayati, Shirley Anugrah  and
      Anastasopoulos, Antonios  and
      Zamaraeva, Olga  and
      Prud'hommeaux, Emily  and
      Child, Jennette  and
      Child, Sara  and
      Knowles, Rebecca  and
      Moeller, Sarah  and
      Micher, Jeffrey  and
      Li, Yiyuan  and
      Zink, Sydney  and
      Xia, Mengzhou  and
      Sharma, Roshan S  and
      Littell, Patrick",
    booktitle = "Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)",
    month = may,
    year = "2020",
    address = "Online",
    publisher = "European Language Resources association",
    url = "https://www.aclweb.org/anthology/2020.sltu-1.48",
    pages = "342--351",
    abstract = "Despite recent advances in natural language processing and other language technology, the application of such technology to language documentation and conservation has been limited. In August 2019, a workshop was held at Carnegie Mellon University in Pittsburgh, PA, USA to attempt to bring together language community members, documentary linguists, and technologists to discuss how to bridge this gap and create prototypes of novel and practical language revitalization technologies. The workshop focused on developing technologies to aid language documentation and revitalization in four areas: 1) spoken language (speech transcription, phone to orthography decoding, text-to-speech and text-speech forced alignment), 2) dictionary extraction and management, 3) search tools for corpora, and 4) social media (language learning bots and social media analysis). This paper reports the results of this workshop, including issues discussed, and various conceived and implemented technologies for nine languages: Arapaho, Cayuga, Inuktitut, Irish Gaelic, Kidaw{'}ida, Kwak{'}wala, Ojibwe, San Juan Quiahije Chatino, and Seneca.",
    language = "English",
    ISBN = "979-10-95546-35-1",
}

@InProceedings{AbrahsGesslerMarge2019Brex,
    title = "{B.} {Rex}: A Dialogue Agent for Book Recommendations",
	  author = {Mitchell Abrams and Luke Gessler and Matthew Marge},
    booktitle = "Proc. of SIGDIAL",
    year = "2019",
    address = "Stockholm, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.sltu-1.48/"
}

@unpublished{GesslerdeRSE19,
title={Optimizing Developer Productivity in Endangered Language Documentation Apps},
author = {Luke Gessler},
year = {2019},
school = {Talk given at deRSE19},
url= {https://derse19.uni-jena.de/derse19/talk/NFAVXV/},
comment={abstract},
url2={https://docs.google.com/presentation/d/1IRiU2z_Xx6QHDA9m_16Gd6pBNjnpnrmlP65AxKyZlyo/edit?usp=sharing},
comment2={slides}
}

@InProceedings{GesslerLiuZeldes2019Signals,
  author    = {Luke Gessler and Yang Liu and Amir Zeldes},
  title     = {A Discourse Signal Annotation System for RST Trees},
  year      = {2019},
  booktitle = {Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019},
  address   = {Minneapolis, Minnesota, USA},
  url = {https://aclweb.org/anthology/papers/W/W19/W19-2708/},
  pages = {56--61},
  comment = {paper}
}

@InProceedings{Gessler2019computel,
 title={Developing without developers: choosing labor-saving tools for language documentation apps},
  author={Luke Gessler},
  journal={Proceedings of the 3rd Workshop on the Use of Computational Methods in the Study of Endangered Languages},
  volume={1},
  pages={6--13},
  year={2019},
  url={https://computel-workshop.org/wp-content/uploads/2019/02/CEL3_book_papers_draft.pdf#page=18},
  comment={paper}
}
</textarea>
</div>
</div>
</div></div></div>
<footer id="postamble" class="">
<div class="container"><div class="row"><div class="col-md-9"><div id="disqus_thread"></div></div></div></div>
</footer>
</body>
</html>
