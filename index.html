<!doctype html>
<html lang="en">
<head>
<title>Luke D. Gessler</title>
<!-- 2023-02-08 Wed 02:26 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="generator" content="Org-mode">
<meta name="author" content="Luke D. Gessler">

<link  href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.5/js/bootstrap.min.js"></script>
<style>
/* org mode styles on top of twbs */

html {
    position: relative;
    min-height: 100%;
}

body {
    font-size: 18px;
    margin-bottom: 105px;
}

footer {
    position: absolute;
    bottom: 0;
    width: 100%;
    height: 101px;
    background-color: #f5f5f5;
}

footer > div {
    padding: 10px;
}

footer p {
    margin: 0 0 5px;
    text-align: center;
    font-size: 16px;
}

#table-of-contents {
    margin-top: 20px;
    margin-bottom: 20px;
}

blockquote p {
    font-size: 18px;
}

pre {
    font-size: 16px;
}

.footpara {
    display: inline-block;
}

figcaption {
  font-size: 16px;
  color: #666;
  font-style: italic;
  padding-bottom: 15px;
}

/* from twbs docs */

.bs-docs-sidebar.affix {
    position: static;
}
@media (min-width: 768px) {
    .bs-docs-sidebar {
        padding-left: 20px;
    }
}

/* All levels of nav */
.bs-docs-sidebar .nav > li > a {
    display: block;
    padding: 4px 20px;
    font-size: 14px;
    font-weight: 500;
    color: #999;
}
.bs-docs-sidebar .nav > li > a:hover,
.bs-docs-sidebar .nav > li > a:focus {
    padding-left: 19px;
    color: #A1283B;
    text-decoration: none;
    background-color: transparent;
    border-left: 1px solid #A1283B;
}
.bs-docs-sidebar .nav > .active > a,
.bs-docs-sidebar .nav > .active:hover > a,
.bs-docs-sidebar .nav > .active:focus > a {
    padding-left: 18px;
    font-weight: bold;
    color: #A1283B;
    background-color: transparent;
    border-left: 2px solid #A1283B;
}

/* Nav: second level (shown on .active) */
.bs-docs-sidebar .nav .nav {
    display: none; /* Hide by default, but at >768px, show it */
    padding-bottom: 10px;
}
.bs-docs-sidebar .nav .nav > li > a {
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 30px;
    font-size: 12px;
    font-weight: normal;
}
.bs-docs-sidebar .nav .nav > li > a:hover,
.bs-docs-sidebar .nav .nav > li > a:focus {
    padding-left: 29px;
}
.bs-docs-sidebar .nav .nav > .active > a,
.bs-docs-sidebar .nav .nav > .active:hover > a,
.bs-docs-sidebar .nav .nav > .active:focus > a {
    padding-left: 28px;
    font-weight: 500;
}

/* Nav: third level (shown on .active) */
.bs-docs-sidebar .nav .nav .nav {
    padding-bottom: 10px;
}
.bs-docs-sidebar .nav .nav .nav > li > a {
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 40px;
    font-size: 12px;
    font-weight: normal;
}
.bs-docs-sidebar .nav .nav .nav > li > a:hover,
.bs-docs-sidebar .nav .nav .nav > li > a:focus {
    padding-left: 39px;
}
.bs-docs-sidebar .nav .nav .nav > .active > a,
.bs-docs-sidebar .nav .nav .nav > .active:hover > a,
.bs-docs-sidebar .nav .nav .nav > .active:focus > a {
    padding-left: 38px;
    font-weight: 500;
}

/* Show and affix the side nav when space allows it */
@media (min-width: 992px) {
    .bs-docs-sidebar .nav > .active > ul {
        display: block;
    }
    /* Widen the fixed sidebar */
    .bs-docs-sidebar.affix,
    .bs-docs-sidebar.affix-bottom {
        width: 213px;
    }
    .bs-docs-sidebar.affix {
        position: fixed; /* Undo the static from mobile first approach */
        top: 20px;
    }
    .bs-docs-sidebar.affix-bottom {
        position: absolute; /* Undo the static from mobile first approach */
    }
    .bs-docs-sidebar.affix .bs-docs-sidenav,.bs-docs-sidebar.affix-bottom .bs-docs-sidenav {
        margin-top: 0;
        margin-bottom: 0
    }
}
@media (min-width: 1200px) {
    /* Widen the fixed sidebar again */
    .bs-docs-sidebar.affix-bottom,
    .bs-docs-sidebar.affix {
        width: 263px;
    }
}
</style>
<script>
$(function() {
    'use strict';

    $('.bs-docs-sidebar li').first().addClass('active');

    $(document.body).scrollspy({target: '.bs-docs-sidebar'});

    $('.bs-docs-sidebar').affix();
});
</script><link rel='stylesheet' type='text/css' href='/css/styles.css'>
            <link href="https://fonts.googleapis.com/css?family=Roboto"
                  rel="stylesheet">
            <script type="text/javascript" src="https://cdn.rawgit.com/pcooksey/bibtex-js/ef59e62c/src/bibtex_js.js"></script>
</head>
<body>
<div id="content" class="container">
<div class="row"><div class="col-md-12"><h1 class="title">Luke D. Gessler</h1>
<img src="img/luke.jpeg" style="float:right;border-radius:50%;width:240px;height:240px;">

<p>
Hi, I'm Luke!
I am in my fifth and final year in the computational linguistics Ph.D. program at <a href="https://linguistics.georgetown.edu">Georgetown University</a>.
I am interested in low-resource NLP and language resource development, particularly in the context of endangered language documentation.
I maintain the <a href="./mala/">Map of Applications for Linguistic Annotation</a>, and I am super into <a href="https://en.wikipedia.org/wiki/Org-mode">org-mode</a> and <a href="https://en.wikipedia.org/wiki/Lisp_(programming_language)">Lisp</a> (<a href="https://en.wikipedia.org/wiki/Clojure">Clojure</a>, in particular).
It is <a href="https://specgram.com/CLXXV.1/03.carlson.cartoon5.html">my duty as a linguist</a> to tell you how many languages I speak, so let me share that they include English (native), Latin (reading), Hindi-Urdu (conversational), Sahidic Coptic (reading), and bits and pieces of others.
</p>

<div id="outline-container-sec-" class="outline-2">
<h2 id="sec-">Research</h2>
<div class="outline-text-2" id="text-">
<p>
Modern methods in natural language processing (NLP) from the past 10 years are powerful but require great amounts of data.
This has led to a performance gap between a handful of high-resource languages which have enough data to fully exploit models' capabilities (like Russian or English), and low-resource languages (like Mohawk or Uyghur), which lack the data volume required to fully realize models' potential.
This is a regrettable circumstance, as poorer performing models may induce many inequities (such as socioeconomic ones), and most of the world's languages may not be spoken by the close of the century and are in need of high-quality language technologies to aid efforts to document and revitalize them.
</p>

<p>
My work is broadly aimed at addressing this performance gap between high- and low-resource languages by developing linguistically-sophisticated resources and algorithms which can help low-resource languages overcome the onerous demands of the deep learning methods which have become dominant in NLP.
Most of my work can be seen as belonging to at least one of the three following threads:
</p>

<ul class="org-ul">
<li>Language resource development: creation and maintenance of natural language corpora enriched with linguistic analyses.
</li>
<li>NLP-capable language documentation systems: developing systems aimed at language documentation, i.e. the process of collecting and describing data in a particular language, with an emphasis on deep integration with NLP systems to facilitate the documentary process.
</li>
<li>Low-resource NLP: development of methods specifically for languages with little data.
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-" class="outline-2">
<h2 id="sec-">Online</h2>
<div class="outline-text-2" id="text-">
<p>
Here is my&#x2026;
</p>
<ul class="org-ul">
<li>Email: lukegessler@gmail.com
</li>
<li><a href="./cv.pdf">CV</a>
</li>
<li><a href="https://github.com/lgessler">GitHub</a>
</li>
<li><a href="https://twitter.com/LukeGessler">Twitter</a>
</li>
<li><a href="https://aclanthology.org/people/l/luke-gessler/">ACL Anthology page</a>
</li>
<li><a href="https://scholar.google.com/citations?user=ppYCkqgAAAAJ&amp;hl=en">Google Scholar page</a>
</li>
<li><a href="https://news.ycombinator.com/user?id=lgessler">Hacker News account</a>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-" class="outline-2">
<h2 id="sec-">Recent work</h2>
<div class="outline-text-2" id="text-">
<div class="bibtex_template">
  <div class="entry">
	<div class="autolist">
	  <div class="if author" style="margin-top:5px;   padding-left: 20px; text-indent: -20px;">
		<span class="author" style="font-weight: bold; "></span>
		<span class="if year">
		  (<span class="year"></span>),
		</span>
		<span class="if journal">"</span><span class="if booktitle">"</span><span class="if school"><span class="title" style="font-style: italic"></span></span><span class="if !school"><span class="title"></span></span><span class="if journal">".</span><span class="if booktitle">". </span><span class="if school">. </span>
		<span class="if journal" style="">
		  <span class="journal" style="font-style:italic"></span>
		  <span class="if volume">
			<span class="volume"></span>
			<span class="if number">
			  (<span class="number"></span>)</span><span class="if pages">, <span class="pages"></span>.
			</span>
		  </span>
		</span>
		<span class="if booktitle" style="">In:
		  <span class="booktitle" style="font-style:italic"></span>.<span class="if series" style=""> (<span class="series" style=""></span>.)</span>
		  <span class="if publisher">
			<span class="publisher"></span>:
		  </span>
		  <span class="if address">
			<span class="address"></span></span><span class="if pages">, <span class="pages"></span></span>.
		</span>
		<span class="if school" style="">
		  <span class="if type"><span class="type"></span>,</span>
		  <span class="school" style=""></span>.
		</span>
		<span class="if url">
		  <br/>[<a class="bibtexVar" href="+url+" style="color:black; font-size:10px" extra="url">
			<span class="if comment">
			  <span class="comment"></span>
			</span>
			<span class="if !comment">
			  <span>link</span>
			</span>
		  </a>]
		</span>
		<span class="if url2">
		  [<a class="bibtexVar" href="+url2+" style="color:black; font-size:10px" extra="url2">
			<span class="if comment2">
			  <span class="comment2"></span>
			</span>
			<span class="if !comment2">
			  <span>link</span>
			</span>
		  </a>]
		</span>
		<span class="if doi">
		  <span class="if !url"><br/></span>[<a class="bibtexVar" href="+doi+" style="color:black; font-size:10px" extra="doi">DOI</a>]
		</span>
	  </div>
	</div>
  </div>
</div>
<div id="bibtex_display"></div>
<textarea id="bibtex_input" style="display:none;">
@inproceedings{gessler-zeldes-2022-microbert,
    title = "{M}icro{BERT}: Effective Training of Low-resource Monolingual {BERT}s through Parameter Reduction and Multitask Learning",
    author = "Gessler, Luke  and
      Zeldes, Amir",
    booktitle = "Proceedings of the The 2nd Workshop on Multi-lingual Representation Learning (MRL)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.mrl-1.9",
    pages = "86--99",
    abstract = "BERT-style contextualized word embedding models are critical for good performance in most NLP tasks, but they are data-hungry and therefore difficult to train for low-resource languages. In this work, we investigate whether a combination of greatly reduced model size and two linguistically rich auxiliary pretraining tasks (part-of-speech tagging and dependency parsing) can help produce better BERTs in a low-resource setting. Results from 7 diverse languages indicate that our model, MicroBERT, is able to produce marked improvements in downstream task evaluations, including gains up to 18{\%} for parser LAS and 11{\%} for NER F1 compared to an mBERT baseline, and we achieve these results with less than 1{\%} of the parameter count of a multilingual BERT base{--}sized model. We conclude that training very small BERTs and leveraging any available labeled data for multitask learning during pretraining can produce models which outperform both their multilingual counterparts and traditional fixed embeddings for low-resource languages.",
}

@inproceedings{gessler-2022-closing,
    title = "Closing the {NLP} Gap: Documentary Linguistics and {NLP} Need a Shared Software Infrastructure",
    author = "Gessler, Luke",
    booktitle = "Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.computel-1.15",
    doi = "10.18653/v1/2022.computel-1.15",
    pages = "119--126",
    abstract = "For decades, researchers in natural language processing and computational linguistics have been developing models and algorithms that aim to serve the needs of language documentation projects. However, these models have seen little use in language documentation despite their great potential for making documentary linguistic artefacts better and easier to produce. In this work, we argue that a major reason for this NLP gap is the lack of a strong foundation of application software which can on the one hand serve the complex needs of language documentation and on the other hand provide effortless integration with NLP models. We further present and describe a work-in-progress system we have developed to serve this need, Glam.",
}

@inproceedings{gessler_midas_2022,
	address = {Marseille, France},
	title = {Midas {Loop}: {A} {Prioritized} {Human}-in-the-{Loop} {Annotation} for {Large} {Scale} {Multilayer} {Data}},
	url = {https://aclanthology.org/2022.lawxvi-1.13},
	abstract = {Large scale annotation of rich multilayer corpus data is expensive and time consuming, motivating approaches that integrate high quality automatic tools with active learning in order to prioritize human labeling of hard cases. A related challenge in such scenarios is the concurrent management of automatically annotated data and human annotated data, particularly where different subsets of the data have been corrected for different types of annotation and with different levels of confidence. In this paper we present [REDACTED], a collaborative, version-controlled online annotation environment for multilayer corpus data which includes integrated provenance and confidence metadata for each piece of information at the document, sentence, token and annotation level. We present a case study on improving annotation quality in an existing multilayer parse bank of English called AMALGUM, focusing on active learning in corpus preprocessing, at the surprisingly challenging level of sentence segmentation. Our results show improvements to state-of-the-art sentence segmentation and a promising workflow for getting "silver" data to approach gold standard quality.},
	booktitle = {Proceedings of {The} 16th {Lingusitic} {Annotation} {Workshop} ({LAW}-{XVI}) within {LREC2022}},
	publisher = {European Language Resources Association},
	author = {Gessler, Luke and Levine, Lauren and Zeldes, Amir},
	month = jun,
	year = {2022},
	pages = {103--110},
}

@inproceedings{gessler_xposition_2022,
	address = {Marseille, France},
	title = {Xposition: {An} {Online} {Multilingual} {Database} of {Adpositional} {Semantics}},
	url = {https://aclanthology.org/2022.lrec-1.194},
	abstract = {We present Xposition, an online platform for documenting adpositional semantics across languages in terms of supersenses (Schneider et al., 2018). More than just a lexical database, Xposition houses annotation guidelines, structured lexicographic documentation, and annotated corpora. Guidelines and documentation are stored as wiki pages for ease of editing, and described elements (supersenses, adpositions, etc.) are hyperlinked for ease of browsing. We describe how the platform structures information; its current contents across several languages; and aspects of the design of the web application that supports it, with special attention to how it supports datasets and standards that evolve over time.},
	booktitle = {Proceedings of the {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Gessler, Luke and Schneider, Nathan and Ledford, Joseph C. and Blodgett, Austin},
	month = jun,
	year = {2022},
	pages = {1824--1830},
}

@inproceedings{uncommon-sense,
    title = "{BERT} has uncommon sense: similarity ranking for word sense {BERTology}",
    author = "Gessler, Luke  and
      Schneider, Nathan",
    booktitle = "Proc. of BlackboxNLP at EMNLP",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2109.09780",
}

@inproceedings{discodisco,
    title = "{DisCoDisCo} at the {DISRPT2021} shared task: a system for discourse segmentation, classification, and connective detection",
    author = "Gessler, Luke  and
      Behzad, Shabnam  and
      Liu, Yang Janet  and
      Peng, Siyao  and
      Zhu, Yilun  and
      Zeldes, Amir",
    booktitle = "Proc. of CODI-DISRPT at EMNLP",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2109.09777",
}

@inproceedings{gessler-etal-2020-supersense,
    title = "Supersense and Sensibility: Proxy Tasks for Semantic Annotation of Prepositions",
    author = "Gessler, Luke  and
      Wein, Shira  and
      Schneider, Nathan",
    booktitle = "Proceedings of LAW at ACL",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.law-1.11",
    pages = "117--126",
    abstract = "Prepositional supersense annotation is time-consuming and requires expert training. Here, we present two sensible methods for obtaining prepositional supersense annotations indirectly by eliciting surface substitution and similarity judgments. Four pilot studies suggest that both methods have potential for producing prepositional supersense annotations that are comparable in quality to expert annotations.",
}

@inproceedings{gessler-etal-2020-amalgum,
    title = "{AMALGUM} {--} A Free, Balanced, Multilayer {E}nglish Web Corpus",
    author = "Gessler, Luke  and
      Peng, Siyao  and
      Liu, Yang  and
      Zhu, Yilun  and
      Behzad, Shabnam  and
      Zeldes, Amir",
    booktitle = "Proc. of LREC",
    month = may,
    year = "2020",
    address = "Online",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.648",
    pages = "5267--5275",
    abstract = "We present a freely available, genre-balanced English web corpus totaling 4M tokens and featuring a large number of high-quality automatic annotation layers, including dependency trees, non-named entity annotations, coreference resolution, and discourse trees in Rhetorical Structure Theory. By tapping open online data sources the corpus is meant to offer a more sizable alternative to smaller manually created annotated data sets, while avoiding pitfalls such as imbalanced or unknown composition, licensing problems, and low-quality natural language processing. We harness knowledge from multiple annotation layers in order to achieve a {``}better than NLP{''} benchmark and evaluate the accuracy of the resulting resource.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{arora-etal-2020-supervised,
    title = "Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in {H}indi and {P}unjabi",
    author = "Arora, Aryaman  and
      Gessler, Luke  and
      Schneider, Nathan",
    booktitle = "Proc. of ACL",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.696",
    doi = "10.18653/v1/2020.acl-main.696",
    pages = "7791--7795",
    abstract = "Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches. We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries. Our best Hindi model achieves state of the art performance, and also achieves good performance on a closely related language, Punjabi, without modification.",
}

@inproceedings{neubig-etal-2020-summary,
    title = "A Summary of the First Workshop on Language Technology for Language Documentation and Revitalization",
    author = "Neubig, Graham  and
      Rijhwani, Shruti  and
      Palmer, Alexis  and
      MacKenzie, Jordan  and
      Cruz, Hilaria  and
      Li, Xinjian  and
      Lee, Matthew  and
      Chaudhary, Aditi  and
      Gessler, Luke  and
      Abney, Steven  and
      Hayati, Shirley Anugrah  and
      Anastasopoulos, Antonios  and
      Zamaraeva, Olga  and
      Prud'hommeaux, Emily  and
      Child, Jennette  and
      Child, Sara  and
      Knowles, Rebecca  and
      Moeller, Sarah  and
      Micher, Jeffrey  and
      Li, Yiyuan  and
      Zink, Sydney  and
      Xia, Mengzhou  and
      Sharma, Roshan S  and
      Littell, Patrick",
    booktitle = "Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)",
    month = may,
    year = "2020",
    address = "Online",
    publisher = "European Language Resources association",
    url = "https://www.aclweb.org/anthology/2020.sltu-1.48",
    pages = "342--351",
    abstract = "Despite recent advances in natural language processing and other language technology, the application of such technology to language documentation and conservation has been limited. In August 2019, a workshop was held at Carnegie Mellon University in Pittsburgh, PA, USA to attempt to bring together language community members, documentary linguists, and technologists to discuss how to bridge this gap and create prototypes of novel and practical language revitalization technologies. The workshop focused on developing technologies to aid language documentation and revitalization in four areas: 1) spoken language (speech transcription, phone to orthography decoding, text-to-speech and text-speech forced alignment), 2) dictionary extraction and management, 3) search tools for corpora, and 4) social media (language learning bots and social media analysis). This paper reports the results of this workshop, including issues discussed, and various conceived and implemented technologies for nine languages: Arapaho, Cayuga, Inuktitut, Irish Gaelic, Kidaw{'}ida, Kwak{'}wala, Ojibwe, San Juan Quiahije Chatino, and Seneca.",
    language = "English",
    ISBN = "979-10-95546-35-1",
}

@InProceedings{AbrahsGesslerMarge2019Brex,
    title = "{B.} {Rex}: A Dialogue Agent for Book Recommendations",
	  author = {Mitchell Abrams and Luke Gessler and Matthew Marge},
    booktitle = "Proc. of SIGDIAL",
    year = "2019",
    address = "Stockholm, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.sltu-1.48/"
}

@unpublished{GesslerdeRSE19,
title={Optimizing Developer Productivity in Endangered Language Documentation Apps},
author = {Luke Gessler},
year = {2019},
school = {Talk given at deRSE19},
url= {https://derse19.uni-jena.de/derse19/talk/NFAVXV/},
comment={abstract},
url2={https://docs.google.com/presentation/d/1IRiU2z_Xx6QHDA9m_16Gd6pBNjnpnrmlP65AxKyZlyo/edit?usp=sharing},
comment2={slides}
}

@InProceedings{GesslerLiuZeldes2019Signals,
  author    = {Luke Gessler and Yang Liu and Amir Zeldes},
  title     = {A Discourse Signal Annotation System for RST Trees},
  year      = {2019},
  booktitle = {Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019},
  address   = {Minneapolis, Minnesota, USA},
  url = {https://aclweb.org/anthology/papers/W/W19/W19-2708/},
  pages = {56--61},
  comment = {paper}
}

@InProceedings{Gessler2019computel,
 title={Developing without developers: choosing labor-saving tools for language documentation apps},
  author={Luke Gessler},
  journal={Proceedings of the 3rd Workshop on the Use of Computational Methods in the Study of Endangered Languages},
  volume={1},
  pages={6--13},
  year={2019},
  url={https://computel-workshop.org/wp-content/uploads/2019/02/CEL3_book_papers_draft.pdf#page=18},
  comment={paper}
}
</textarea>
</div>
</div>
</div></div></div>
<footer id="postamble" class="">
<div class="container"><div class="row"><div class="col-md-9"><div id="disqus_thread"></div></div></div></div>
</footer>
</body>
</html>